## ğŸ‘ï¸ GuardianEye â€“ Smart Eyewear for the Visually Impaired

> An AI-powered, sensor-integrated wearable device for obstacle detection, text recognition, navigation, and real-time assistanceâ€”built to empower visually impaired individuals with safe, independent mobility.

---

![GuardianEye Banner](https://img.shields.io/badge/Smart-Wearable-brightgreen) ![License](https://img.shields.io/badge/license-MIT-blue) ![Status](https://img.shields.io/badge/status-Prototype-orange)

## ğŸ” Overview

**GuardianEye** is a multi-sensory smart eyewear system designed to assist visually impaired individuals in navigating their surroundings confidently and independently. By combining advanced **AI**, **computer vision**, and **sensor fusion**, the device offers:

- Real-time **obstacle detection**
- **Currency note recognition**
- **Printed text reading** via OCR
- **Multi-language voice feedback**
- Location tracking for caregivers
- Hands-free voice command control
- **Emergency alert** system with haptic and audio feedback

---

## ğŸš€ Key Features

| Feature                        | Description                                                                 |
|-------------------------------|-----------------------------------------------------------------------------|
| ğŸ›£ï¸ Real-Time Obstacle Detection | Detects and classifies static/dynamic obstacles using **LiDAR + YOLO**     |
| ğŸ’¬ Multi-Language Voice Output | NLP-based output in **multiple languages** for localized interaction        |
| ğŸ“· Optical Character Recognition (OCR) | Reads text from newspapers, signboards, etc., and converts to speech |
| ğŸ™ï¸ Voice Command System        | Microphone-powered **hands-free interaction** via natural language          |
| ğŸ’¸ Currency Note Detection     | Identifies denomination using computer vision to prevent fraud             |
| ğŸ§­ GPS Location Tracking       | Enables **family members** to track userâ€™s location remotely                |
| ğŸ¤– Person & Object Recognition | Recognizes known individuals and frequently seen objects                    |
| ğŸ“³ Haptic Feedback             | Vibrates for obstacle alerts, turns, or important environmental cues       |
| ğŸš¨ Emergency Beep Alert       | Loud **beeping sound** triggers when immediate danger is detected          |

---

## ğŸ§  Technologies Used

- **ğŸ§  AI Models:** YOLOv8 (Object Detection), Tesseract (OCR), NLP (Speech-to-Text / Text-to-Speech)
- **ğŸŒ Communication:** Bluetooth Low Energy (BLE)
- **ğŸ”Š Audio Feedback:** Bone conduction speakers
- **ğŸ“· Sensors:** LiDAR, Pi camera, GPS
- **ğŸ“¦ Hardware Platform:** Microcontroller (Raspberry Pi), Rechargeable battery, Microphone

---

## ğŸ§° System Architecture

```

User Commands â”€â”€â–º Microphone
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      AI Processing Unit     â”‚
â”‚  (YOLO, OCR, NLP, Sensor Fusion) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LiDAR Sensor â”‚   â”‚  Camera       â”‚   â”‚  GPS Module   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â–¼                  â–¼                  â–¼
Real-Time Obstacle   Text Recognition   Location Tracking
Detection & Feedback  + Object ID       + Caregiver Alert

Output:
ğŸ”Š Voice Feedback (Multi-Language, TTS)
ğŸ“³ Haptic Vibrations
ğŸš¨ Emergency Beep Alert

```

---

## ğŸ¯ Use Cases

- Navigate urban streets, stairs, and corridors
- Recognize people at work or home
- Read public signs or packaging labels
- Detect currency during monetary exchange
- Alert surroundings in case of emergencies
- Stay connected with caregivers for safety

---

## ğŸ“· Media & Demos

> Coming Soon: Demo Video 

- âœ… Real-time object detection
- âœ… Text-to-speech conversion of printed material
- âœ… GPS tracking dashboard for caregivers
- âœ… Voice-command feature demo

---

## ğŸ§ª Project Status

- [x] Research and literature review
- [x] Hardware component selection
- [x] AI module prototyping (YOLO + OCR)
- [x] NLP-based multi-language system
- [ ] Integration & Testing
- [ ] Real-world trials with visually impaired users
- [ ] Final production and optimization

---


## ğŸ‘¨â€ğŸ’» Contributors

- **Anirudh Garg** â€“ Computer vision(Team Lead)
- **Aaradhya Sharma** â€“ Computer vision 
- **Abhiroop Singh** â€“ IoT 
- **Rajveer Singh** â€“ Speech Processing, Documentation  
- **Bhavneet Kaur** â€“ NLP, Speech Processing 

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ¤ Support & Contact

Have feedback, ideas, or want to collaborate? Reach out at:

ğŸ“§ agarg3_be22@thapar.edu  
ğŸŒ [Thapar Institute of Engineering & Technology](https://www.thapar.edu)

---

> _"Letâ€™s build a world where everyone can see possibilitiesâ€”even without sight."_ ğŸŒ
